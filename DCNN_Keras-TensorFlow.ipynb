{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Name of dataset is not found: D://KT_dataset/cifarpascal",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1b9ea5a108ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m     46\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-1b9ea5a108ef>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(unused_argv)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munused_argv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m   \u001b[1;31m# Загружаем данные для обучения и оценки\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m   \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D://KT_dataset/cifarpascal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m   \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[1;31m# массив numpy для данных обучения\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m   \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Приводим к целочисленным значениям\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\__init__.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(name, size, test_with_fake_data)\u001b[0m\n\u001b[0;32m     67\u001b[0m   \"\"\"\n\u001b[0;32m     68\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDATASETS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Name of dataset is not found: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'dbpedia'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mDATASETS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_with_fake_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Name of dataset is not found: D://KT_dataset/cifarpascal"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/tutorials/layers\n",
    "# Импорт библиотек\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Импорт библиотеки для работы с массивами и тензорфлоу\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Настройка выводимой в логах информации, аналог vebrose\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def cnn_model_fn(features, labels, mode): # Построение нейронной сети\n",
    "    \n",
    "  # Входной слой, размер уменьшен до 32х32, как у CIFAR-10\n",
    "  input_layer = tf.reshape(features[\"x\"], [-1, 32, 32, 1])\n",
    "\n",
    "  # Сверточный слой 1\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # Слой пулинга 1\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Сверточный слой 2\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  \n",
    "  # Слой пулинга 2\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "  # Сверточный слой 3\n",
    "  conv3 = tf.layers.conv2d(\n",
    "      inputs=pool2,\n",
    "      filters=128,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  \n",
    "  # Слой пулинга 3\n",
    "  pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    " \n",
    "  # Разверточный слой 1\n",
    "  deconv1 = tf.layers.deconv2d(\n",
    "      inputs=pool3,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  \n",
    "  # Слой анпулинга 1\n",
    "  pre_unpool1 = tf.layers.max_pooling2d(inputs=deconv1, pool_size=[2, 2], strides=2)\n",
    "  unpool1 = unpool(pre_unpool1, ind, kernel_size=(1, 2, 2, 1), scope='unpool')\n",
    "    \n",
    "  # Разверточный слой 2\n",
    "  deconv2 = tf.layers.deconv2d(\n",
    "      inputs=unpool1,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "  \n",
    "  # Слой пулинга 3\n",
    "  pre_unpool2 = tf.layers.max_pooling2d(inputs=deconv2, pool_size=[2, 2], strides=2)\n",
    "  unpool2 = unpool(pre_unpool2, ind, kernel_size=(1, 2, 2, 1), scope='unpool')\n",
    " \n",
    "\n",
    "  # Изменение размерности входящего слоя, аналог Flatten()\n",
    "  unpool2_flat = tf.reshape(unpool2, [-1, 7 * 7 * 64])\n",
    "    \n",
    "  # Полносвязный слой и дропаут\n",
    "  dense = tf.layers.dense(inputs=unpool2_flat, units=128, activation=tf.nn.relu)\n",
    "  dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  # Слой-классификатор с 10 нейронами для 10 классов\n",
    "  logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "  # Формирование результатов\n",
    "  predictions = {\n",
    "      # Определение класса для каждого объекта\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # \n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  # Для обеих моделей подсчитваем потери методом категориальной кросс энтропии\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "  # Настройка оптимизатора для модели, стохастический градиентный спуск\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN: # Если набор данных - тренировочный\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001) # Тренируем сеть с использованием SGD\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Добавление метрики \n",
    "  eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "# Слой анпулинга, взято из https://github.com/tensorflow/tensorflow/issues/2169#issuecomment-216607417\n",
    "def unpool(pool, ind, ksize=(1, 2, 2, 1), scope='unpool'):\n",
    "    \"\"\"\n",
    "       Unpooling layer after max_pool_with_argmax.\n",
    "       Args:\n",
    "           pool:   max pooled output tensor\n",
    "           ind:      argmax indices (produced by tf.nn.max_pool_with_argmax)\n",
    "           ksize:     ksize is the same as for the pool\n",
    "       Return:\n",
    "           unpooled:    unpooling tensor\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        pooled_shape = pool.get_shape().as_list()\n",
    "\n",
    "        flatten_ind = tf.reshape(ind, (pooled_shape[0], pooled_shape[1] * pooled_shape[2] * pooled_shape[3]))\n",
    "        # sparse indices to dense ones_like matrics\n",
    "        one_hot_ind = tf.one_hot(flatten_ind,  pooled_shape[1] * ksize[1] * pooled_shape[2] * ksize[2] * pooled_shape[3], on_value=1., off_value=0., axis=-1)\n",
    "        one_hot_ind = tf.reduce_sum(one_hot_ind, axis=1)\n",
    "        one_like_mask = tf.reshape(one_hot_ind, (pooled_shape[0], pooled_shape[1] * ksize[1], pooled_shape[2] * ksize[2], pooled_shape[3]))\n",
    "        # resize input array to the output size by nearest neighbor\n",
    "        img = tf.image.resize_nearest_neighbor(pool, [pooled_shape[1] * ksize[1], pooled_shape[2] * ksize[2]])\n",
    "        unpooled = tf.multiply(img, tf.cast(one_like_mask, img.dtype))\n",
    "        return unpooled\n",
    "\n",
    "def main():\n",
    "  # Загружаем данные для обучения и оценки\n",
    "  dataset = tf.contrib.learn.datasets.load_dataset(\"D://KT_dataset/cifarpascal\")\n",
    "  train_data = dataset.train.images # массив numpy для данных обучения\n",
    "  train_labels = np.asarray(dataset.train.labels, dtype=np.int32) # Приводим к целочисленным значениям\n",
    "  eval_data = dataset.test.images # массив numpy для данных оценки\n",
    "  eval_labels = np.asarray(dataset.test.labels, dtype=np.int32) # Приводим к целочисленным значениям\n",
    "    \n",
    "  # Добавляем Estimator - класс в TensorFlow для высокоуровнего обучения и оценки моделей\n",
    "  classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=\"/testmodel/\") \n",
    "\n",
    "  # Периодическое занесение данных в лог\n",
    "  tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "  logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "  # Общие настройки данных для обучения\n",
    "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": train_data},\n",
    "    y=train_labels,\n",
    "    batch_size=100,\n",
    "    num_epochs=35,\n",
    "    shuffle=True)\n",
    "  classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=20000,\n",
    "    hooks=[logging_hook])\n",
    "\n",
    "  # Общие настройки данных для обучения\n",
    "  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "  print(eval_results)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "  #tf.app.run()\n",
    "  main()"
   ]
  }
  }
